1. [FM因子分解机](#fm因子分解机)
2. [CLUE](#clue)
3. [CONTENT](#content)
	1. [特征稀疏性](#特征稀疏性)
	2. [交叉特征](#交叉特征)
	3. [线性模型](#线性模型)
	4. [二阶多项式核](#二阶多项式核)
	5. [因子分解机模型](#因子分解机模型)
	6. [效率问题](#效率问题)
4. [SUMMARY](#summary)

## FM因子分解机
## CLUE
- 特征稀疏性
- 交叉特征
- 线性模型
- 二阶多项式核 SVM
- 因子分解机模型
- 效率问题
## CONTENT
### 特征稀疏性
在诸如 CTR 预估、推荐或搜索 ranking 的场景中，特征是非常稀疏的。特征的稀疏性往往来自分类特征的 one-hot 编码。
### 交叉特征
用户是否会点击某个 item，往往与不同特征的组合高度相关。对于这些特征，对产品形态和策略较熟悉的工程师，可以根据这些先验知识，进行人工的特征组合，作为新的组合特征交付给模型使用。不过，当分类特征增多，特别是取值多的分类特征越来越多，进行人工特征交叉的工作量会越来越大。此外，全凭经验的特征工程，可能无法完全捕捉到特征中蕴含的规律，从而降低模型预测性能的天花板。这是这类做法的主要缺陷之一。
### 线性模型
这里说的线性模型指的是线性回归和逻辑回归模型。单纯的线性模型无法捕获交叉特征。
### 二阶多项式核
为两两的特征组合分配一个权重参数。
$$\hat y = f(\vec x) = w_0 + \sum_{i = 1}^{n}w_ix_i + \sum_{0 < i < j <= n}w_{i, j}x_ix_j.$$
这实际上就是核函数选择为二阶多项式核的 SVM 模型。
### 因子分解机模型
这样设计的模型看起来能够学习到特征两两交叉带来的信息了，但是二阶多项式核SVM泛化性能不足。原因是$w_{i,j}$的取值完全取决于$x_i$和$x_j$的乘积。
FM 模型的解决办法是为每个维度的特征（$x_i$）学习一个表征向量（$v_i$，其实可以理解为是特征 ID 的 embedding 向量）。
而后将 $x_i$ 和 xj 的乘积的权重设定为各自表征向量的点积。也就是有如下形式的预测函数：
$$\hat y = f(\vec x) = w_0 + \sum_{i = 1}^{n}w_ix_i + \sum_{0 < i < j <= n}\langle \vec v_i, \vec v_j\rangle x_ix_j.$$
为什么相对二阶多项式核 SVM 做出的改进能够提高模型的泛化性能？
哪怕在训练集中，$x_ix_j$始终为零，其参数$\langle \vec v_i, \vec v_j\rangle$也是经过了学习更新的，因此能够表现出很好的泛化性能。
### 效率问题
$$\begin{aligned}
\hat y
  &{} = f(\vec x) \\
  &{} = w_0 + \sum_{i = 1}^{n}w_ix_i + \frac{1}{2}\sum_{d = 1}^{k}\biggl(\Bigl(\sum_{i = 1}^{n}\vec v_{i, d}x_i\Bigr)^2 - \sum_{i = 1}^{n}\vec v_{i, d}^2x_i^2\biggr).
\end{aligned}$$
它的复杂度是$O(kn)$。考虑到特征的稀疏性，尽管$n$可能很大，但很多$xi$都是零。因此其实际复杂度应该是$O(k\bar n)$——其中$\bar n$表示样本不为零的特征维度数量的平均值。


<a href="https://www.cnblogs.com/pinard/p/6370127.html">分解机(Factorization Machines)推荐算法原理</a>\
<a href="https://liam.page/2019/03/25/Factorization-Machine/">谈谈因子分解机模型（FM）</a>
## SUMMARY
FM 模型不仅在模型本身能够满足下列两个特性，还保证了训练和预测的效率为 O(kn¯)，因而是非常优秀的模型、被广泛运用：
- 能够处理大规模的稀疏数据，并保有足够好的泛化性能（generalization performance）；
- 同时，能够自动地学习到特征交叉带来的信息。
