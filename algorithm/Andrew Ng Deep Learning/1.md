logistic回归是最简单的深度学习。
$$\hat{y}^i=\sigma(\boldsymbol{w}^Tx^{(i)}+b),\sigma(z^{(i)})=\frac{1}{1+e^{-z^{(i)}}}$$
损失函数：
$$L(\hat{y}^{(i)},y^{(i)})=-y^{(i)}\log\hat{y}^{(i)}-(1-y^{(i)})\log{(1-\hat{y}^{(i)})}\tag{1.1}$$
风险函数：
$$J(\boldsymbol{w},b)=-\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})\tag{1.2}$$
式子(1.2)是凸函数，可以使用梯度下降法：
$$w:=w-\alpha\frac{dJ(w,b)}{dw}\tag{1.3}$$
$$b:=b-\alpha\frac{dJ(w,b)}{db}\tag{1.3}$$
流程图可以简化理解：正向传播可以求值，反向传播可以求导。
尽可能不要使用for循环，而要使用向量，在python中：
