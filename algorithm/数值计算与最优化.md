## TITLE
## CLUE
## CONTENT
### 简介
上实际中期形成和发展起来的一门应用数学分支科学。
机器学习的模型训练，最终都归结为最优化问题，也就是寻找最优的参数，使得模型的误差损失函数最小，而寻找最优参数的方法称为最优化方法。
### 无约束极小值的最优化条件
定理：局部极小值点必有梯度等于零。

利用机制的必要条件，把求解函数极小值问题，转化为求解下面的方程组问题：
$$\nabla{f(\boldsymbol{x})}=0$$
但是一般不会用解析法来求解上式而是使用数值计算最常用的最优化方法迭代法。它的基本思想是：首先给定$f(\boldsymbol{x})$的一个极小值点的初始估计$x^{0}$，然后通过迭代的方法得到点序列$\{x^t\}(t=1,2,...)$，如果这个点序列的极限$x^*$逼近极小值点，那么称这个序列为极小化序列，因此，最优化问题转化为应该如何得到这个极小化的点序列？

在欧几里得空间中，点$x^{k+1}$和点$x^{k+1}$之差是一个向量，写成：
$$x^{k+1}=x^{k}+\lambda\boldsymbol{d}_k$$
$\lambda$为学习率，$\boldsymbol{d}_k$为一个迭代方向。好的迭代算法必须满足两个条件：第一，递减行，第二，收敛性。
### 梯度下降算法
梯度下降算法是神经网络最常用的优化方法之一。
#### 梯度下降算法的分类
根据每一次迭代所使用的训练数据集范围的不同，可以分为：
##### 批量梯度下降(BGD)
也称为最速下降法，误差损失函数由全量训练数据的误差构成。
###### 缺点
- 当数据量很大时，速度会非常慢。
- 它不能以在线的方式更新模型，即当训练数据有新的元素加入时，需要对全量的数据进行更新，因此效率很低。
- 因此一般不被采用。
##### 随机梯度下降(SGD)
每次更新只考虑一个样本数据的误差损失，故其优点就是速度远快于BGD，且能进行在线的参数更新。
###### 缺点
- 由于单个样本会出现相似或重复的情况，因此数据的参数更新会出现冗余。
- 单个数据之间的差异会比较大，造成每一次迭代的损失函数会出现较大的波动。
<div align="center"><img src="https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=2267645611,32278770&fm=173&app=25&f=JPG?w=544&h=282&s=FC8B793287525DCA50DD70D3030080B4" height="200"><p>图1.1</p></div>

##### 小批量梯度下降(MBGD)
结合了BGD和SGD的优点，并且克服了它们的缺点，每次的参数更新，优化的目标函数是由$n$个样本数据构成，而$n$一般很小，一般取$10$到$500$之间。
###### 优点
- 效率跟SGD差别不大。
- 与SGD相比，MBGD每一批考虑了更多的样本数据，每一批数据之间的整体差异更小、更平均，结果也更稳定。
- 与SGD一样适用于在线的模型更新。
#### 传统更新策略
亦被称为**vanilla策略**，梯度下降算法中最简单的参数更新策略。
$$\theta=\theta-lr×d\theta$$
##### 优点
简单高效，学习率较小时，能够保证得到全局最优解（凸函数）或者局部最优解（非凸函数）。
##### 缺点
- 学习率太小，算法收敛速度过慢；学习率过大，在迭代过程中容易出现震荡现象，甚至无法收敛。
- 在较为平坦的平面区域，由于梯度接近于0，因此每一次迭代的变化非常小，造成训练效率下降，甚至被误判为最优解而提前终止。
<div align="center"><img src="./statics/传统梯度下降算法的方向变化图.jpg" height="200"><p>图1.1</p></div>
#### 动量更新策略
传统的梯度更新策略，每一次迭代的方向$d_k$等于当前的batch数据集的误差损失函数的梯度，但是，很多时候不同的batch之间数据变化比较大，这样就会造成迭代方向来回震荡的情况，从而减缓了收敛的速度，甚至导致发散。

动量更新策略，更新时在一定程度上保留之前等新的方向的同时，也利用当前batch的梯度微调最终的更新方向，因此，整个更新策略及保留了稳定性，也能根据实际的数据变化做出相应的调整。
$$v_i=mu×v_{i-1}+lr×d(\theta^{i-1})\\
\theta^i=\theta^{i-1}-v_i$$
第一部分是上一时刻的迭代方向，即动量或惯性，记为$mu×v_{i-1}$，$v_{i-1}$为上意识可的迭代方向，$mu$是动量系数。
第二部分是当前batch样本集合的梯度方向，记为$lr×d(\theta^{i-1})$。
当上一次迭代方向与当前batch的梯度方向相反时，动量更新策略能起到一个减速作用，防止走偏；相反，如果相同，则起到一个加速的作用，**避免在平坦的曲面上迭代过慢**。
<div align="center"><img src="./statics/动量更新策略的方向变化图.jpg" height="200"><p>图1.2</p></div>

#### 改进的动量更新策略
$$(\theta^{i-1})'=\theta^{i-1}-mu×v_{i-1}\\
v_i=mu×v_{i-1}+lr×d((\theta^{i-1})')\\
\theta^i=\theta^{i-1}-v_i$$
这种被称为"向前看"策略效率更高。
<div align="center"><img src="./statics/改进的动量更新策略的方向变化图.jpg" height="200"><p>图1.3</p></div>

#### 自适应梯度策略
## SUMMARY
