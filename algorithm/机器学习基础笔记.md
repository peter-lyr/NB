## TITLE
## CLUE
## CONTENT
### 线性模型
#### 定义
线性模型(linear model)，是机器学习中的一类算法总称，其形式化定义为：通过给定的样本数据集D，线性模型试图学习到这样的一个模型，使得对于任意的输入特征向量$\boldsymbol{x}=(x_1,x_2,...,x_n)$，模型的预测输出$f(\boldsymbol{x})$能够表示为输入特征向量$\boldsymbol{x}$的先行函数，即满足：<font color=red>啥</font>
$$f(\boldsymbol{x})=w_1x_1+w_2x_2+...+w_nx_n+b$$
#### 线性回归
##### 基本方程
损失函数为均方误差：
$$L(w,b)=\frac{1}{2m}\sum_{i=1}^m\left(f(x^{(i)})-y^{(i)}\right)^2$$
最小二乘法解得：
$$\boldsymbol\theta=(\boldsymbol{X}\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y},\boldsymbol\theta=(w_1,w_2,...,w_n,b)^T$$
##### 局部加权线性回归
一种改进方法，没有挖掘更多的特征，比如不同特征之间的组合，可以解决欠拟合的问题。缺点之一就是当通过训练得到参数最优解后，除了需要保留最优参数外，还需保留全部训练数据，以求取没一个训练数据对应于新输入数据的权重值。这里提到一个概念，那就是高斯核。
损失函数：
$$L(w,b)=\frac{1}{2m}\sum_{i=1}^m\mu^{(i)}\left(f(x^{(i)})-y^{(i)}\right)^2$$
最优解：
$$\boldsymbol\theta=(\boldsymbol{X}\boldsymbol\psi\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol\psi\boldsymbol{Y}$$
其中，$\boldsymbol\psi$是一个对角矩阵，满足：
$$\boldsymbol\psi_{i,i}=\mu^{(i)}=\exp\left(-\frac{(x^{(i)}-x)^2}{2\tau^2}\right)\tag{1.1}$$
其中，$\tau$越大，则欠拟合越厉害，越小则过拟合越厉害，所以需要折中。而式子（1.1）就是高斯核（核函数的一种）。
##### 最小二乘法和高斯分布的关系
利用最小二乘法来拟合训练数据，等价于吧训练数据的暑促看成是服从高斯分布。
#### logistic回归
##### 损失函数
$$J(w,b)=-\sum_{i=1}^m\left(y^{(i)}\ln{f(x^{(i)})}+(1-y)\ln\left(1-f(x^{(i)})\right)\right)\tag{1.2}$$
式子(1.2)中等号右侧大括号内的表达式是交叉熵，也是对数似然函数，并且它是一个凸函数，可以利用梯度下降算法求解。
##### 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？
损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。
将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。
这个式子的更新速度和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。
为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。
sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。
##### 逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
训练完以后，每一个特征的权重值都是原来特征权重值的百分之一。
##### 为什么我们还是会在训练的过程当中将高度相关的特征去掉？
去掉高度相关的特征会让模型的可解释性更好，可以大大提高训练的速度。
##### 优点
1. 适合需要得到一个分类概率的场景。
2. 计算代价不高，容易理解实现。LR在时间和内存需求上相当高效。它可以应用于分布式数据，并且还有在线算法实现，用较少的资源处理大型数据。
3. LR对于数据中小噪声的鲁棒性很好，并且不会受到轻微的多重共线性的特别影响。（严重的多重共线性则可以使用逻辑回归结合L2正则化来解决，但是若要得到一个简约模型，L2正则化并不是最好的选择，因为它建立的模型涵盖了全部的特征。）
##### 缺点
1. 容易欠拟合，分类精度不高。
2. 数据特征有缺失或者特征空间很大时表现效果并不好。
##### 与SVM比较
线性回归做分类因为考虑了所有样本点到分类决策面的距离，所以在两类数据分布不均匀的时候将导致误差非常大；LR和SVM克服了这个缺点，其中LR将所有数据采用sigmod函数进行了非线性映射，使得远离分类决策面的数据作用减弱；SVM直接去掉了远离分类决策面的数据，只考虑支持向量的影响。

但是对于这两种算法来说，在线性分类情况下，如果异常点较多无法剔除的话，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献；SVM+软间隔对异常比较敏感，因为其训练只需要支持向量，有效样本本来就不高，一旦被干扰，预测结果难以预料。
##### 样本太大怎么处理？
1. 对特征离散化，离散化后用one-hot编码处理成0,1值，再用LR处理会较快收敛；
2. 如果一定要用连续值的话，可以做scaling；
3. 工具的话有 spark Mllib，它损失了一小部分的准确度达到速度的提升；
4. 如果没有并行化平台，想做大数据就试试采样。需要注意采样数据，最好不要随机取，可以按照日期/用户/行为，来分层抽样。
##### 怎么使样本平衡？
1. 如果样本不均衡，样本充足的情况下可以做下采样——抽样，样本不足的情况下做上采样——对样本少的做重复；
2. 修改损失函数，给不同权重。比如负样本少，就可以给负样本大一点的权重；
3. 采样后的predict结果，用作判定请还原。
##### 关于特征处理
1. 离散化优点：映射到高维空间，用linear的LR(快，且兼具更好的分割性)；稀疏化，0,1向量内积乘法运算速度快，计算结果方 便存储，容易扩展；离散化后，给线性模型带来一定的非线性；模型稳定，收敛度高，鲁棒性好；在一定程度上降低了过拟合风险
2. 通过组合特征引入个性化因素：比如uuid+tag
3. 注意特征的频度： 区分特征重要度，可以用重要特征产出层次判定模型
##### 算法调优
假设只看模型的话：
1. 选择合适的正则化：L2准确度高，训练时间长；L1可以做一定的特征选择，适合大量数据
2. 收敛阈值e，控制迭代轮数
3. 样本不均匀时调整loss function,给不同权重
4. Bagging或其他方式的模型融合
5. 选择最优化算法：liblinear、sag、newton-cg等
#### 广义线性模型
其实上面两种线性模型都是广义线性模型的特殊形式。
### 支持向量机
#### 关于
1993年提出，1995年发表，
#### 函数间隔和几何间隔
我们希望支持向量的集合间隔最小，由此我们得到带约束的最优化问题：
$$\max\ \frac{1}{2}||w||^2\\
s.t.\ y^{(i)}(w^Tx^{(i)}+b)\ge1\ \ i=1,2,...,m\tag{2.1}$$
式子(2.1)就是要求解的SVM最优化模型，由其最优解得到的分类器，称为最优间隔分类器。式子(2.1)也是SVM最优化的基本型。
#### 对偶问题
引入拉格朗日乘子$\alpha_i,i=1,2,...,m$，构建拉格朗日函数：
$$L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^m\alpha_i\left[y^{(i)}(w^Tx^{(i)}+b)-1\right]\tag{2.2}$$
将拉格朗日函数分别对$w$和$b$求偏导，并令其为零，得：
$$w=\sum_{i=1}^m\alpha_iy^{(i)}x^{(i)}\\
\sum_{i=1}^m\alpha_iy^{(i)}=0\tag{2.3}$$
将式子(2.3)带入式子(2.2)，得只含$\alpha$的函数：
$$L(\alpha)=\sum_{i=1}^m\alpha-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j{x^{(i)}}^Tx^{(j)}$$
转换为对偶问题：
$$\max\ \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j{x^{(i)}}^Tx^{(j)}-\sum_{i=1}^m\alpha_i\\
s.t.\ \sum_{i=1}^m\alpha_iy^{(i)}=0\ i=1,2,...,m\\
\alpha_i\ge0\ i=1,2,...,m\tag{2.4}$$
#### 序列最小优化法SMO
对于式子(2.4)，可以用常规的二次规划最优化来求解，带是对偶问题存在更高效的解法。
## SUMMARY
