1. 什么是归一化，它与标准化的区别是什么？
2. 如何确定 CNN 的卷积核通道数和卷积输出层的通道数？
3. 什么是卷积？
4. 什么是CNN的池化pool层？
5. 简述下什么是生成对抗网络
6. 学梵高作画的原理是什么？
7. 请简要介绍下tensorflow的计算图
8. 你有哪些深度学习（rnn、cnn）调参的经验？
9. CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？
10. LSTM结构推导，为什么比RNN好？
11. Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不足，有没改进的激活函数。
12. 为什么引入非线性激励函数？
13. 请问人工神经网络中为什么ReLu要好过于tanh和sigmoid function？
14. 如何解决RNN梯度爆炸和弥散的问题？
15. 什么样的数据集不适合用深度学习？
16. 广义线性模型是怎被应用在深度学习中？
17. 如何缓解梯度消失和梯度膨胀（微调、梯度截断、改良激活函数等）
18. 简述神经网络的发展历史
19. 深度学习常用方法
20. 请简述神经网络的发展史。
21. 神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的？
22. 梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？
23. 简单说说CNN常用的几个模型
24. 为什么很多做人脸的Paper会最后加入一个Local Connected Conv？
25. 什么是梯度爆炸？
26. 梯度爆炸会引发什么问题？
27. 如何确定是否出现梯度爆炸？
28. 如何修复梯度爆炸问题？
29. LSTM神经网络输入输出究竟是怎样的？
30. 什么是RNN？
31. 请详细介绍一下RNN模型的几种经典结构
32. 简单说下sigmoid激活函数
33. 如何从RNN起步，一步一步通俗理解LSTM
34. 请详细说说CNN的工作原理
35. CNN究竟是怎样一步一步工作的？
36. rcnn、fast-rcnn和faster-rcnn三者的区别是什么
37. 在神经网络中，有哪些办法防止过拟合？
38. CNN是什么，CNN关键的层有哪些？
39. GRU是什么？GRU对LSTM做了哪些改动？
40. 如何解决深度学习中模型训练效果不佳的情况？
41. 神经网络中，是否隐藏层如果具有足够数量的单位，它就可以近似任何连续函数？
42. 为什么更深的网络更好？ 
43. 更多的数据是否有利于更深的神经网络？ 
44. 不平衡数据是否会摧毁神经网络？ 
45. 你如何判断一个神经网络是记忆还是泛化? 
46. 无监督降维提供的是帮助还是摧毁？ 
47. 是否可以将任何非线性作为激活函数? 
48. 批大小如何影响测试正确率？ 
49. 损失函数重要吗？ 
50. 初始化如何影响训练? 
51. 不同层的权重是否以不同的速度收敛？ 
52. 正则化如何影响权重？ 
53. 什么是fine-tuning？
54. 什么是边框回归Bounding-Box regression，以及为什么要做、怎么做
55. 请阐述下Selective Search的主要思想
56. 什么是非极大值抑制（NMS）？
57. 什么是深度学习中的anchor？
58. CNN的特点以及优势 
59. 深度学习中有什么加快收敛/降低训练难度的方法？
60. 请简单说下计算流图的前向和反向传播
61. 请写出链式法则并证明
62. 请写出Batch Normalization的计算方法及其应用
63. 神经网络中会用到批量梯度下降（BGD）吗？为什么用随机梯度下降（SGD）?
64. 当神经网络的调参效果不好时，从哪些角度思考？（不要首先归结于overfiting）
65. 请阐述下卷积神经网络CNN的基本原理(完善版)
66. 了解无人驾驶的核心技术么？
67. 如何形象的理解LSTM的三个门
68. 通过一张张动图形象的理解LSTM
69. 如何理解反向传播算法BackPropagation
70. 请问什么是softmax函数？
71. 通俗理解BN(Batch Normalization)
72. 批量归一化BN到底解决了什么问题？
73. 如何理解随机梯度下降，以及为什么SGD能够收敛？
74. 模拟退火算法能解决陷入局部最优的问题么
75. 请说下常见优化方法各自的优缺点（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）
76. Adam 算法的原理机制是怎么样的？它与相关的 AdaGrad 和 RMSProp 方法有什么区别
