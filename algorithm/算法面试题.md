
1. 请详细说说 **支持向量机** （support vector machine，SVM）的原理
2. 哪些机器学习算法不需要做 **归一化处理** ？
  > <a target="_blank" href="https://www.cnblogs.com/bjwu/p/8977141.html">机器学习数据预处理——标准化/归一化方法</a>，讲解了标准化和归一化的两个好处，它们之间的对比。
  >
  > <a target="_blank" href="https://zhuanlan.zhihu.com/p/29957294">ML 入门：归一化、标准化和正则化</a>，加了一个正则化，所以顺带讲解了范数等概念。
  >
  > <a target="_blank" href="https://blog.csdn.net/JonyHwang/article/details/80983468">哪些机器学习算法不需要做归一化</a>，概率模型（树形模型）不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、SVM、LR、Knn、KMeans之类的最优化问题就需要归一化。
3. 树形结构为什么不需要归一化？
  > <a target="_blank" href="https://zhuanlan.zhihu.com/p/65591873">树形结构为什么不需要归一化，树模型为什么是不能进行梯度下降</a>，概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。所以树模型（回归树）寻找最优点是通过寻找最优分裂点完成的
4. 在 **k-means** 或 **kNN** ，我们常用 **欧氏距离** 来计算最近的邻居之间的距离，有时也用 **曼哈顿距离** ，请对比下这两种距离的差别
  > <a target="_blank" href="https://www.cnblogs.com/pinard/p/6164214.html">K-Means聚类算法原理</a>， <u>**k-means**</u> 无监督的聚类算法，最小化平方误差，但它是NP难问题，故采用迭代方法。传统K-Means $\to$ K-Means++(初始化优化) $\to$ elkan K-Means(距离计算优化) $\to$ Mini Batch K-Means(大样本优化)。
  >
  > <a target="_blank" href="https://blog.csdn.net/zsl091125/article/details/76984532">KNN算法详解</a>，k近邻分类(k-nearest neighbor classification)算法，优点：简单，易于理解，易于实现，无需估计参数，无需训练，缺点：懒惰算法，对测试样本分类时的计算量大，内存开销大，评分慢可解释性较差，无法给出决策树那样的规则。
  >
  > 计算步骤：
  > 1. 算距离：给定测试对象，计算它与训练集中的每个对象的距离；
  > 2. 找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻；
  > 3. 做分类：根据这k个近邻归属的主要类别，来对测试对象分类。
  >
  > 常见问题：
  > 1. k值设定为多大？
  > 2. 类别如何判定最合适？
  > 3. 如何选择合适的距离衡量？
  > 4. 训练样本是否要一视同仁？
  > 5. 性能问题？6. 能否大幅减少训练样本量，同时又保持分类精度？
  >
  > <a target="_blank" href="https://www.julyedu.com/question/big/kp_id/23/ques_id/926">答案详解</a>
  > 欧式距离将样本的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。
5. 数据 **归一化** （或者标准化，注意归一化和标准化不同）的原因
  > <a target="_blank" href="https://blog.csdn.net/pipisorry/article/details/52247379">数据标准化/归一化normalization</a>
  >
  > 目标：
  > 1. 把数变为（0，1）之间的小数
  > 2. 把有量纲表达式变为无量纲表达式
  >
  > 好处：
  > 1. 提升模型的收敛速度
  > 2. 提升模型的精度
6. 请简要说说一个完整机器学习项目的流程
  > <a target="_blank" href="https://zhuanlan.zhihu.com/p/62879598">答案解析</a>
  >
  > 1. 抽象成数学问题
  > 2. 获取数据
  > 3. 特征预处理与特征选择
  > 4. 训练模型与调优
  > 5. 模型诊断
  > 6. 模型融合
  > 7. 上线运行
7. **逻辑斯特回归** 为什么要对特征进行 **离散化** 。
  > <a href="https://zhuanlan.zhihu.com/p/61049356">答案解析</a>
  >
  > 1. 非线性
  > 2. 速度快
  > 3. 鲁棒性
  > 4. 方便交叉与特征组合
  > 5. 稳定性
  > 6. 简化模型
8. 简单介绍下 **LR**
9. **overfitting** 怎么解决
10. LR和 **SVM** 的联系与区别
11. 什么是 **熵**
12. 说说 **梯度下降法**
13. **牛顿法** 和梯度下降法有什么不同？
14. **熵** 、 **联合熵** 、 **条件熵** 、 **相对熵** 、 **互信息** 的定义
15. 说说你知道的 **核函数**
16. 什么是 **拟牛顿法** （Quasi-Newton Methods）？
17. **kmeans的复杂度** ？
18. 请说说 **随机梯度下降法** 的问题和挑战？
19. 说说 **共轭梯度法** ？
20. 对所有 **优化问题** 来说, 有没有可能找到比現在已知算法更好的算法？
21. 什么是 **最大熵**
22. **LR与线性回归的区别与联系**
23. 简单说下 **有监督学习和无监督学习的区别**
24. 请问（ **决策树、Random** **Forest、Boosting、Adaboot** **）GBDT和XGBoost** 的区别是什么？
25. 机器学习中的 **正则化** 到底是什么意思？
26. 说说 **常见的损失函数** ？
27. 为什么 **xgboost要用泰勒展开** ，优势在哪里？
28. **协方差和相关性** 有什么区别？
29. **xgboost**如何 **寻找最优特征** ？是有放回还是无放回的呢？
30. 谈谈 **判别式模型** 和 **生成式模型** ？
31. 线性分类器与非线性分类器的区别以及优劣
32. L1和L2的区别
33. L1和L2正则先验分别服从什么分布
34. 简单介绍下logistics回归？
35. 说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式。
36. 为什么朴素贝叶斯如此“朴素”？
37. 请大致对比下plsa和LDA的区别
38. 请详细说说EM算法
39. KNN中的K如何选取的？
40. 防止过拟合的方法
41. 机器学习中，为何要经常对数据做归一化
42. 什么最小二乘法？
43. 梯度下降法找到的一定是下降最快的方向么？
44. 简单说说贝叶斯定理
45. 怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感。
46. 请举例说明什么是标准化、归一化
47. 随机森林如何处理缺失值？
48. 随机森林如何评估特征重要性？
49. 请说说Kmeans的优化？
50. KMeans初始类簇中心点的选取。
51. 解释对偶的概念。
52. 如何进行特征选择？
53. 衡量分类器的好坏？
54. 机器学习和统计里面的auc的物理意义是啥？
55. 数据预处理。
56. 观察增益gain, alpha和gamma越大，增益越小？
57. 什麽造成梯度消失问题?
58. 到底什么是特征工程？
59. 你知道有哪些数据处理和特征工程的处理？
60. 准备机器学习面试应该了解哪些理论知识？
61. 数据不平衡问题
62. 特征比数据量还大时，选择什么样的分类器？
63. 常见的分类算法有哪些？他们各自的优缺点是什么？
64. 常见的监督学习算法有哪些？
65. 说说常见的优化算法及其优缺点？
66. 特征向量的归一化方法有哪些？
67. RF与GBDT之间的区别与联系？
68. <img src="http://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514889198_863.png">
69. 请比较下EM算法、HMM、CRF
70. 带核的SVM为什么能分类非线性问题？
71. 请说说常用核函数及核函数的条件
72. 请具体说说Boosting和Bagging的区别
73. 逻辑回归相关问题
74. 什么是共线性, 跟过拟合有什么关联?
75. 机器学习中，有哪些特征选择的工程方法？
76. 用贝叶斯机率说明Dropout的原理
77. 对于维度极低的特征，选择线性还是非线性分类器？
78. 请问怎么处理特征向量的缺失值
79. SVM、LR、决策树的对比。
80. 什么是ill-condition病态问题？
81. 常用的聚类划分方式有哪些？列举代表算法。
82. 什么是偏差与方差？
83. 解决bias和Variance问题的方法是什么？
84. 采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？
85. xgboost怎么给特征评分？
86. 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？
87. 推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到
88. 请写出你了解的机器学习特征工程操作，以及它的意义
89. 请写出你对VC维的理解和认识
90. kmeans聚类中，如何确定k的大小
91. 请用Python实现下线性回归，并思考下更高效的实现方式
92. 怎么理解“机器学习的各种模型与他们各自的损失函数一一对应？”
93. 给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？
94. 给你一个癌症检测的数据集。你已经建好了分类模型，取得了96％的精度。为什么你还是不满意你的模型性能？你可以做些什么呢？
95. 解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？
96. 你意识到你的模型受到低偏差和高方差问题的困扰。应该使用哪种算法来解决问题呢？为什么？
97. KNN和KMEANS聚类（kmeans clustering）有什么不同？
98. 真阳性率和召回有什么关系？写出方程式。
99. 什么时候Ridge回归优于Lasso回归？
100. 全球平均温度的上升导致世界各地的海盗数量减少。这是否意味着海盗的数量减少引起气候变化？
101. 如何在一个数据集上选择重要的变量？给出解释。
102. 运行二元分类树算法很容易，但是你知道一个树是如何做分割的吗，即树如何决定把哪些变量分到哪个根节点和后续节点上？
103. 你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？
104. 你有一个数据集，变量个数p大于观察值个数n。为什么用OLS是一个不好的选择？用什么技术最好？为什么？
105. 给你一个缺失值多于30%的数据集？比方说，在50个变量中，有8个变量的缺失值都多于30%。你对此如何处理？
106. “买了这个的客户，也买了......”亚马逊的建议是哪种算法的结果？
107. 你怎么理解第一类和第二类错误？
108. 当你在解决一个分类问题时，出于验证的目的，你已经将训练集随机抽样地分成训练集和验证集。你对你的模型能在未看见的数据上有好的表现非常有信心，因为你的验证精度高。但是，在得到很差的精度后，你大失所望。什么地方出了错？
109. 在应用机器学习算法之前纠正和清理数据的步骤是什么？
110. 什么是K-means聚类算法？
111. 如何理解模型的过拟合与欠拟合，以及如何解决？
112. 请详细说说文字特征提取
113. 请详细说说图像特征提取
114. 了解xgboost么，请详细说说它的原理
115. 请详细说说梯度提升树(GBDT)的原理
116. 请说说Adaboost 算法的原理与推导
117. 机器学习中的L0、L1与L2范数到底是什么意思？
118. 怎么确定LDA的topic个数？
119. 连续特征，既可以离散化，也可以做幅度缩放，那这两种处理方式分别适用于什么场景呢？
120. 从几何直观的角度解释下为什么拉格朗日乘子法能取到最优值？
121. A/B测试的数学原理与深入理解
122. 如何更科学的做机器学习100天入门计划
123. 如何通俗理解主成成分分析PCA
124. 机器学习/数据挖掘中如何处理缺失值
125. 如何通俗理解LightGBM
126. 线性回归要求因变量服从正态分布？
127. 什么是K近邻算法和KD树？
128. 如何通俗理解贝叶斯方法和贝叶斯网络？
129. 最大熵模型中的数学推导
