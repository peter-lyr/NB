1. 请详细说说 **支持向量机** （support vector machine，SVM）的原理
2. 哪些机器学习算法不需要做 **归一化处理** ？
> <a href="https://www.cnblogs.com/bjwu/p/8977141.html">机器学习数据预处理——标准化/归一化方法</a>，讲解了标准化和归一化的两个好处，它们之间的对比。\
> <a href="https://zhuanlan.zhihu.com/p/29957294">ML 入门：归一化、标准化和正则化</a>，加了一个正则化，所以顺带讲解了范数等概念。\
> <a href="https://blog.csdn.net/JonyHwang/article/details/80983468">哪些机器学习算法不需要做归一化</a>，概率模型（树形模型）不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、RF。而像Adaboost、SVM、LR、Knn、KMeans之类的最优化问题就需要归一化。
3. 树形结构为什么不需要归一化？
> <a href="https://zhuanlan.zhihu.com/p/65591873">树形结构为什么不需要归一化，树模型为什么是不能进行梯度下降</a>，概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。所以树模型（回归树）寻找最优点是通过寻找最优分裂点完成的
4. 在 **k-means** 或 **kNN** ，我们常用 **欧氏距离** 来计算最近的邻居之间的距离，有时也用 **曼哈顿距离** ，请对比下这两种距离的差别
5. 数据归一化（或者标准化，注意归一化和标准化不同）的原因
6. 请简要说说一个完整机器学习项目的流程
7. 逻辑斯特回归为什么要对特征进行离散化。
8. 简单介绍下LR
9. overfitting怎么解决
10. LR和SVM的联系与区别
11. 什么是熵
12. 说说梯度下降法
13. 牛顿法和梯度下降法有什么不同？
14. 熵、联合熵、条件熵、相对熵、互信息的定义
15. 说说你知道的核函数
16. 什么是拟牛顿法（Quasi-Newton Methods）？
17. kmeans的复杂度？
18. 请说说随机梯度下降法的问题和挑战？
19. 说说共轭梯度法？
20. 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法？
21. 什么是最大熵
22. LR与线性回归的区别与联系
23. 简单说下有监督学习和无监督学习的区别
24. 请问（决策树、Random Forest、Boosting、Adaboot）GBDT和XGBoost的区别是什么？
25. 机器学习中的正则化到底是什么意思？
26. 说说常见的损失函数？
27. 为什么xgboost要用泰勒展开，优势在哪里？
28. 协方差和相关性有什么区别？
29. xgboost如何寻找最优特征？是有放回还是无放回的呢？
30. 谈谈判别式模型和生成式模型？
31. 线性分类器与非线性分类器的区别以及优劣
32. L1和L2的区别
33. L1和L2正则先验分别服从什么分布
34. 简单介绍下logistics回归？
35. 说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式。
36. 为什么朴素贝叶斯如此“朴素”？
37. 请大致对比下plsa和LDA的区别
38. 请详细说说EM算法
39. KNN中的K如何选取的？
40. 防止过拟合的方法
41. 机器学习中，为何要经常对数据做归一化
42. 什么最小二乘法？
43. 梯度下降法找到的一定是下降最快的方向么？
44. 简单说说贝叶斯定理
45. 怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感。
46. 请举例说明什么是标准化、归一化
47. 随机森林如何处理缺失值？
48. 随机森林如何评估特征重要性？
49. 请说说Kmeans的优化？
50. KMeans初始类簇中心点的选取。
51. 解释对偶的概念。
52. 如何进行特征选择？
53. 衡量分类器的好坏？
54. 机器学习和统计里面的auc的物理意义是啥？
55. 数据预处理。
56. 观察增益gain, alpha和gamma越大，增益越小？
57. 什麽造成梯度消失问题?
58. 到底什么是特征工程？
59. 你知道有哪些数据处理和特征工程的处理？
60. 准备机器学习面试应该了解哪些理论知识？
61. 数据不平衡问题
62. 特征比数据量还大时，选择什么样的分类器？
63. 常见的分类算法有哪些？他们各自的优缺点是什么？
64. 常见的监督学习算法有哪些？ 
65. 说说常见的优化算法及其优缺点？
66. 特征向量的归一化方法有哪些？
67. RF与GBDT之间的区别与联系？
68. <img src="http://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1514889198_863.png">
69. 请比较下EM算法、HMM、CRF
70. 带核的SVM为什么能分类非线性问题？ 
71. 请说说常用核函数及核函数的条件
72. 请具体说说Boosting和Bagging的区别
73. 逻辑回归相关问题
74. 什么是共线性, 跟过拟合有什么关联?
75. 机器学习中，有哪些特征选择的工程方法？
76. 用贝叶斯机率说明Dropout的原理
77. 对于维度极低的特征，选择线性还是非线性分类器？
78. 请问怎么处理特征向量的缺失值
79. SVM、LR、决策树的对比。
80. 什么是ill-condition病态问题？
81. 常用的聚类划分方式有哪些？列举代表算法。
82. 什么是偏差与方差？
83. 解决bias和Variance问题的方法是什么？
84. 采用 EM 算法求解的模型有哪些，为什么不用牛顿法或梯度下降法？
85. xgboost怎么给特征评分？
86. 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？
87. 推导朴素贝叶斯分类 P(c|d)，文档 d（由若干 word 组成），求该文档属于类别 c 的概率， 并说明公式中哪些概率可以利用训练集计算得到
88. 请写出你了解的机器学习特征工程操作，以及它的意义
89. 请写出你对VC维的理解和认识
90. kmeans聚类中，如何确定k的大小
91. 请用Python实现下线性回归，并思考下更高效的实现方式
92. 怎么理解“机器学习的各种模型与他们各自的损失函数一一对应？”
93. 给你一个数据集，这个数据集有缺失值，且这些缺失值分布在离中值有1个标准偏差的范围内。百分之多少的数据不会受到影响？为什么？
94. 给你一个癌症检测的数据集。你已经建好了分类模型，取得了96％的精度。为什么你还是不满意你的模型性能？你可以做些什么呢？
95. 解释朴素贝叶斯算法里面的先验概率、似然估计和边际似然估计？
96. 你意识到你的模型受到低偏差和高方差问题的困扰。应该使用哪种算法来解决问题呢？为什么？
97. KNN和KMEANS聚类（kmeans clustering）有什么不同？
98. 真阳性率和召回有什么关系？写出方程式。
99. 什么时候Ridge回归优于Lasso回归？
100. 全球平均温度的上升导致世界各地的海盗数量减少。这是否意味着海盗的数量减少引起气候变化？
101. 如何在一个数据集上选择重要的变量？给出解释。
102. 运行二元分类树算法很容易，但是你知道一个树是如何做分割的吗，即树如何决定把哪些变量分到哪个根节点和后续节点上？
103. 你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？
104. 你有一个数据集，变量个数p大于观察值个数n。为什么用OLS是一个不好的选择？用什么技术最好？为什么？
105. 给你一个缺失值多于30%的数据集？比方说，在50个变量中，有8个变量的缺失值都多于30%。你对此如何处理？
106. “买了这个的客户，也买了......”亚马逊的建议是哪种算法的结果？
107. 你怎么理解第一类和第二类错误？
108. 当你在解决一个分类问题时，出于验证的目的，你已经将训练集随机抽样地分成训练集和验证集。你对你的模型能在未看见的数据上有好的表现非常有信心，因为你的验证精度高。但是，在得到很差的精度后，你大失所望。什么地方出了错？
109. 在应用机器学习算法之前纠正和清理数据的步骤是什么？
110. 什么是K-means聚类算法？
111. 如何理解模型的过拟合与欠拟合，以及如何解决？
112. 请详细说说文字特征提取
113. 请详细说说图像特征提取
114. 了解xgboost么，请详细说说它的原理
115. 请详细说说梯度提升树(GBDT)的原理
116. 请说说Adaboost 算法的原理与推导
117. 机器学习中的L0、L1与L2范数到底是什么意思？
118. 怎么确定LDA的topic个数？
119. 连续特征，既可以离散化，也可以做幅度缩放，那这两种处理方式分别适用于什么场景呢？
120. 从几何直观的角度解释下为什么拉格朗日乘子法能取到最优值？
121. A/B测试的数学原理与深入理解
122. 如何更科学的做机器学习100天入门计划
123. 如何通俗理解主成成分分析PCA
124. 机器学习/数据挖掘中如何处理缺失值
125. 如何通俗理解LightGBM
126. 线性回归要求因变量服从正态分布？
127. 什么是K近邻算法和KD树？
128. 如何通俗理解贝叶斯方法和贝叶斯网络？
129. 最大熵模型中的数学推导
